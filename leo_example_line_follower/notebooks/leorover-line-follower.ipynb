{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3509232,"sourceType":"datasetVersion","datasetId":2111897}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport tqdm\n\nimport cv2\nimport keras as K\nimport keras.layers as KL\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.utils import Sequence\nfrom tqdm.keras import TqdmCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Variables that need to be changed if you are training on your data","metadata":{}},{"cell_type":"code","source":"# name of root directory of the dataset (name of the zip file with your dataset)\nroot_dir = \"full-track\"\n\n# lower and upper bound for capturing color mask\nhsv_lower = (94, 38, 65)\nhsv_upper = (136, 141, 172)\n\n# height of the crop area (number of pixels, starting from upper edge of image that will be cropped)\ncrop_height = 220\n\n# path to test image for preprocessing (can be any image)\npreprocess_path = \"../input/full-track/data/21420221938-img1.jpg\"\n\n# save directory for trained model\nsave_dir = \"./model.keras\"\n\n# Chose the name of saved tflite model\ntflite_model_name = \"tflite_model\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading prepared data labels and partition","metadata":{}},{"cell_type":"code","source":"labels = None\npartition = None\n\n\nwith open('../input/' + root_dir + '/labels.pickle', 'rb') as handle:\n    labels = pickle.load(handle)\n    \nwith open('../input/' + root_dir + '/partition.pickle', 'rb') as handle:\n    partition = pickle.load(handle)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"{elem.split(\"-\")[0] for elem in partition['validation']}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network model parts","metadata":{}},{"cell_type":"code","source":"GPUs_num = len(tf.config.list_physical_devices('GPU'))\nCPUs_num = len(tf.config.list_physical_devices('CPU'))\nprint(\"Num GPUs Available: \", GPUs_num)\nprint(\"gpu_devices: \", tf.config.list_physical_devices('GPU'))\nprint(\"Num CPUs Available: \", CPUs_num)\nprint(\"cpu_devices: \", tf.config.list_physical_devices('CPU'))\ndevice = None\ndevice = '/GPU:0' if GPUs_num > 0 else '/CPU:0'\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess function","metadata":{}},{"cell_type":"code","source":"def preprocess(img, dim):\n    # converting to hsv\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    # croping the img\n    crop_img = hsv_img[crop_height:hsv_img.shape[0], :]\n    # catching color mask\n    color_mask = cv2.inRange(crop_img, hsv_lower, hsv_upper)\n    # conveting values to float\n    float_img = color_mask.astype(np.float32)\n    # resizing\n    resized_img = cv2.resize(float_img, (dim[1], dim[0]))\n    # normalizing\n    final_img = resized_img / 255.0\n    \n    return final_img[:,:,np.newaxis]\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing test","metadata":{}},{"cell_type":"code","source":"preprocess_test_img = cv2.imread(preprocess_path)\nprint(f\"Image shape before preprocess: {preprocess_test_img.shape}\")\nplt.imshow(cv2.cvtColor(preprocess_test_img, cv2.COLOR_BGR2RGB))\nplt.show()\nprocessed = preprocess(preprocess_test_img, (120,160))\nprint(f\"Image shape after preprocess: {processed.shape}\")\nplt.imshow(processed, cmap=\"gray\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Keras Data Generator","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1, shuffle=True, **kwargs):\n        'Initialization'\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n    \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples'\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y_linear = np.empty((self.batch_size, 1), dtype=float)\n        y_angular = np.empty((self.batch_size, 1), dtype=float)\n\n        for i, ID in enumerate(list_IDs_temp):\n            img = cv2.imread('../input/' + root_dir + '/data/' + ID)\n            preprocess_img = preprocess(img, self.dim)\n            X[i,:] = preprocess_img\n\n            y_linear[i] = self.labels['linear'][ID]\n            y_angular[i] = self.labels['angular'][ID]\n        \n        # return  {'img_in': X}, {'linear': y_linear, 'angular': y_angular}\n        return {'img_in': X}, (y_linear, y_angular)\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Keras model\n","metadata":{}},{"cell_type":"code","source":"img_in = KL.Input(shape=(120, 160, 1), name='img_in')\nx = img_in\n\nx = KL.Convolution2D(filters=24, kernel_size=(5, 5), strides=(2, 2), activation='relu')(x)\nx = KL.Convolution2D(filters=32, kernel_size=(5, 5), strides=(2, 2), activation='relu')(x)\nx = KL.Convolution2D(filters=64, kernel_size=(5, 5), strides=(2, 2), activation='relu')(x)\nx = KL.Convolution2D(filters=64, kernel_size=(3, 3), strides=(2, 2), activation='relu')(x)\nx = KL.Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu')(x)\n\nx = KL.Flatten(name='flattened')(x)\nx = KL.Dense(units=100, activation='linear')(x)\nx = KL.Dropout(rate=.1)(x)\nx = KL.Dense(units=50, activation='linear')(x)\nx = KL.Dropout(rate=.1)(x)\n\nlinear = KL.Dense(units=1, activation='linear', name='linear')(x)\n\nangular = KL.Dense(units=1, activation='linear', name='angular')(x)\n\nmodel = K.Model(inputs=[img_in], outputs=[linear, angular])\n\nwith tf.device(device):\n    model.compile(optimizer='adam',\n                  loss={'linear': 'mean_squared_error', 'angular': 'mean_squared_error'},\n                  # loss=['mean_squared_error', 'mean_squared_error'],\n                  loss_weights={'linear': 0.3, 'angular': 0.7})\n                  # loss_weights=[0.3, 0.7])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {'dim': (120, 160),\n          'batch_size': 64,\n          'n_channels': 1,\n          'shuffle': True}\n\n\nwith tf.device(device):\n    training_generator = DataGenerator(partition['train'], labels, **params)\n    params['shuffle'] = False\n    validation_generator = DataGenerator(partition['validation'], labels, **params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"callbacks = [\n        K.callbacks.ModelCheckpoint(save_dir, save_best_only=True),\n        K.callbacks.EarlyStopping(monitor='val_angular_loss',\n                                  min_delta=.0001,\n                                  patience=15,\n                                  verbose=True,\n                                  mode='min',\n                                  restore_best_weights=True),\n        TqdmCallback(verbose=1),\n        K.callbacks.ReduceLROnPlateau(monitor='val_angular_loss', factor=0.8,\n                              patience=5, min_lr=0.001, mode='min', verbose=1),\n        K.callbacks.TensorBoard(log_dir='./logs', profile_batch=(0, 10))\n\n\n    ]\nwith tf.device(device):\n    hist = model.fit(training_generator,\n                     validation_data=validation_generator,\n                     callbacks=callbacks,\n                     epochs=100,\n                     verbose=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Converting Keras model to tflite","metadata":{}},{"cell_type":"markdown","source":"TensorFlow Lite reverses the order of multi-output tensors during conversion.\nTo maintain backward compatibility with the legacy ROS1 model (where the first output was linear speed and the second was angular speed), we create a temporary Keras model with its outputs intentionally swapped before converting it to TFLite.\nAfter conversion, TFLite reverses the outputs again, restoring the correct order for the ROS node while still supporting the old models.\n\nTherefore, the current ROS2 node can use both the legacy model and any new models trained with this notebook.","metadata":{}},{"cell_type":"code","source":"reorder_model = K.Model(inputs=[img_in], outputs=[model.output[1], model.output[0]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(reorder_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT] \ntflite_model = converter.convert()\n\nwith open(tflite_model_name + '.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom tests","metadata":{}},{"cell_type":"code","source":"# get prefixes of images names\nfiles = os.listdir(\"../input/\" + root_dir + \"/data\")\nimage_amount_dict = dict()\nfor file in files:\n    div = file.split(\"-\")\n    current_max = image_amount_dict.get(div[0], -1)\n    current_index = int(div[1][3:-4])\n    image_amount_dict[div[0]] = max(current_max, current_index)\nprint(\"Available prefixes and number of images for each prefix:\")\nimage_amount_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_file = \"21420221946-img750.jpg\" # can be any image of the data directory\ntest_dimension = (120, 160)\ntest_img = cv2.imread(\"../input/\" + root_dir + \"/data/\" + test_file)\n\nplt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"Shape of the test image: {}\".format(test_img.shape))\n\npreprocessed_test = preprocess(test_img, test_dimension)\nprint(\"Shape of the preprocessed test image: {}\".format(preprocessed_test.shape))\nplt.imshow(preprocessed_test, cmap=\"gray\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparing predictions","metadata":{}},{"cell_type":"code","source":"# KERAS\nkeras_input = np.empty((1, 120, 160, 1))\nkeras_input[0,:] = preprocessed_test\nkeras_prediction = model.predict({'img_in': keras_input})\n\n# REORDERED KERAS \nreord_keras_prediction = reorder_model.predict({'img_in': keras_input})\n\n\n# TFLITE\ninterpreter = tf.lite.Interpreter(model_path=tflite_model_name+'.tflite')\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninterpreter.allocate_tensors()\n\ninterpreter.set_tensor(input_details[0]['index'], [preprocessed_test])\n    \ninterpreter.invoke()\n\ntflite_prediction = [interpreter.get_tensor(output_details[0]['index'])[0][0],\n                     interpreter.get_tensor(output_details[1]['index'])[0][0]]\n\n# GROUND TRUTH\nground_truth = [labels['linear'][test_file], labels['angular'][test_file]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = {\n    'Keras prediction': [out.item() for out in keras_prediction],\n    'Reordered Keras prediction': [out.item() for out in reord_keras_prediction],\n    'TFLite prediction': tflite_prediction,\n    'Ground Truth': ground_truth\n}\nindex_labels = ['Linear', 'Angular']\n\n# Create the DataFrame\ndf = pd.DataFrame(data, index=index_labels)\n\n# Display with some styling\nstyled_df = df.style.format(\"{:.2f}\").set_table_styles([\n    {'selector': 'th', 'props': [('background-color', '#4C72B0'),\n                                 ('color', 'white'),\n                                 ('font-weight', 'bold'),\n                                 ('text-align', 'center')]},\n    {'selector': 'td', 'props': [('text-align', 'center'),\n                                 ('padding', '8px'),\n                                 ('border', '1px solid #ddd')]},\n    {'selector': 'tr:nth-child(even)', 'props': [('background-color', '#f9f9f9')]}\n]).set_caption(\"Results\")\n\nstyled_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Model features","metadata":{}},{"cell_type":"code","source":"def get_conv_layers(trained_model):\n    num = 0\n    nums = []\n    for layer in trained_model.layers:\n        if 'conv' in layer.name:\n            nums.append(num)\n        num += 1\n    return nums","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_kernels(layer_num=1, trained_model=model, filters_num=6):\n    '''\n        layer_num - number of convolutional layer which kernels will be visualized\n        trained_model - name of the model that we want to visualize filters from\n        filters_num - number of filters to visualize\n    '''\n    conv_layers_ids = get_conv_layers(trained_model)\n    \n    if layer_num not in conv_layers_ids:\n        print(\"layer_num is not an index of convolutional layer!\")\n        print(\"Indexes of conv layers:\")\n        print(conv_layers_ids)\n        return\n    filters, biases = trained_model.layers[layer_num].get_weights()\n    filter_min, filter_max = filters.min(), filters.max()\n    \n    filters = (filters - filter_min) / (filter_max - filter_min)\n    max_filters_num = filters.shape[3]\n    if filters_num > max_filters_num:\n        print(\"Number of filters to visualize greater than number of filters in chosen conv layer\")\n        print(f\"Number of filters in chosen layer: {max_filters_num}\")\n        return\n        \n    ix = 1\n    for i in range(filters_num):\n        f = filters[:,:,:,i]\n        for j in range(1):\n            ax = plt.subplot(filters_num, 3, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            plt.imshow(f[:,:,j], cmap='gray')\n            ix += 1\n        \n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_feature_maps(layer_num=1, base_model=model, input_img=keras_input, row=8, column=3):\n    '''\n        layer_num - number of convolutional layer which features will be visualized\n        base_model - name of the model that we want to visualize features from\n        input_img - input img for the base model to show its features\n        row - number of maps in a row\n        column - number of maps in a column\n    '''\n    conv_layers_ids = get_conv_layers(base_model)\n    if layer_num not in conv_layers_ids:\n        print(\"layer_num is not an conv layer id\")\n        print(\"Conv layers ids:\", conv_layers_ids)\n        return\n    \n    visualization_model = K.models.Model(inputs=base_model.inputs, outputs=model.layers[layer_num].output)\n    feature_maps = visualization_model.predict(input_img)\n    maps_num = feature_maps.shape[3]\n    \n    if row * column > maps_num:\n        print(\"Specified plot row and column exceed number of feature maps from chosen layer\")\n        print(f\"Number of maps in conv layer {layer_num} is {maps_num}\")\n        print(f\"row * column = {row*column}\")\n        return\n    \n    ix = 1\n    for _ in range(row):\n        for _ in range(column):\n            ax = plt.subplot(column, row, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n            ix += 1\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_kernels(layer_num=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_feature_maps(column=4, row=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}